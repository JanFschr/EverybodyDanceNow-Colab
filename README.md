### |  ðŸ§¾ Colab Notebook  | : [Link] (https://colab.research.google.com/github/justinjohn0306/EverybodyDanceNow-Colab/blob/master/Justin_EverybodyDanceNow.ipynb) 




# Everybody Dance Now

### [[website]](https://carolineec.github.io/everybody_dance_now/) [[paper]](https://arxiv.org/pdf/1808.07371.pdf) [[youtube]](https://www.youtube.com/watch?v=PCBTZh41Ris)

![alt text](https://laughingsquid.com/wp-content/uploads/2018/08/Everybody-Dance-Now.gif)

Implementation accompanying paper:  
Everybody Dance Now  
Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros  
UC Berkeley  
hosted on arXiv










```

## Citation

If you find this work useful please use the following citation:

```
@inproceedings{chan2019dance,
 title={Everybody Dance Now},
 author={Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A},
 booktitle={IEEE International Conference on Computer Vision (ICCV)},
 year={2019}
}
```

## Acknowledgements

Model code adapted from [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)

Data Preparation code adapted from [Realtime_Multi-Person_Pose_Estimation](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation)

Data Preparation code based on outputs from [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
